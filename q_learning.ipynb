{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class QLearning:\n",
    "    \"\"\"\n",
    "    QLearning reinforcement learning agent.\n",
    "\n",
    "    Arguments:\n",
    "      epsilon - (float) The probability of randomly exploring the action space\n",
    "        rather than exploiting the best action.\n",
    "      discount - (float) The discount factor. Controls the perceived value of\n",
    "        future reward relative to short-term reward.\n",
    "      adaptive - (bool) Whether to use an adaptive policy for setting\n",
    "        values of epsilon during training\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, epsilon=0.2, discount=0.95, adaptive=False):\n",
    "        self.epsilon = epsilon\n",
    "        self.discount = discount\n",
    "        self.adaptive = adaptive\n",
    "\n",
    "    def fit(self, env, steps=1000):\n",
    "        \"\"\"\n",
    "        Trains an agent using Q-Learning on an OpenAI Gym Environment.\n",
    "\n",
    "        Arguments:\n",
    "          env - (Env) An OpenAI Gym environment with discrete actions and\n",
    "            observations. See the OpenAI Gym documentation for example use\n",
    "            cases (https://gym.openai.com/docs/).\n",
    "          steps - (int) The number of actions to perform within the environment\n",
    "            during training.\n",
    "\n",
    "        Returns:\n",
    "          state_action_values - (np.array) The values assigned by the algorithm\n",
    "            to each state-action pair as a 2D numpy array. The dimensionality\n",
    "            of the numpy array should be S x A, where S is the number of\n",
    "            states in the environment and A is the number of possible actions.\n",
    "          rewards - (np.array) A 1D sequence of averaged rewards of length 100.\n",
    "            Let s = np.floor(steps / 100), then rewards[0] should contain the\n",
    "            average reward over the first s steps, rewards[1] should contain\n",
    "            the average reward over the next s steps, etc.\n",
    "        \"\"\"\n",
    "        state_action_values = np.zeros(\n",
    "            (env.observation_space.n, env.action_space.n))\n",
    "        N_actions_performed = np.zeros((env.action_space.n, ), dtype=int)\n",
    "        state = env.reset()\n",
    "        rewards = np.zeros((100, ))\n",
    "\n",
    "        s = np.floor(steps / 100)\n",
    "        s_count = 0\n",
    "        reward_sum = 0\n",
    "        idx = 0\n",
    "\n",
    "        for step in range(steps):\n",
    "            epsilon = self._get_epsilon(step / steps)\n",
    "            # generate random num\n",
    "            p = np.random.random()\n",
    "            # check probability\n",
    "            action = env.action_space.sample(\n",
    "            )  # your agent here (this takes random actions)\n",
    "            if p >= epsilon and len(set(state_action_values[state])) != 1:\n",
    "                action = np.argmax(state_action_values[state])\n",
    "            # take action and observe R, S'\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            # update values\n",
    "            N_actions_performed[action] += 1\n",
    "            state_action_values[state][\n",
    "                action] += 1 / N_actions_performed[action] * (\n",
    "                    reward +\n",
    "                    self.discount * max(state_action_values[observation]) -\n",
    "                    state_action_values[state][action])\n",
    "            reward_sum += reward\n",
    "            # set next state\n",
    "            state = observation\n",
    "            # check s\n",
    "            s_count += 1\n",
    "            if s == s_count:\n",
    "                rewards[idx] = reward_sum / (step + 1)\n",
    "                s_count = 0\n",
    "                idx += 1\n",
    "\n",
    "            if done:\n",
    "                state = env.reset()\n",
    "\n",
    "        return state_action_values, rewards\n",
    "\n",
    "    def predict(self, env, state_action_values):\n",
    "        \"\"\"\n",
    "        Runs prediction on an OpenAI environment usinz the policy defined by\n",
    "        the QLearning algorithm and the state action values. Predictions are\n",
    "        run for exactly one episode. Note that one episode may produce a\n",
    "        variable number of steps.\n",
    "\n",
    "        Arguments:\n",
    "          env - (Env) An OpenAI Gym environment with discrete actions and\n",
    "            observations. See the OpenAI Gym documentation for example use\n",
    "            cases (https://gym.openai.com/docs/).\n",
    "          state_action_values - (np.array) The values assigned by the algorithm\n",
    "            to each state-action pair as a 2D numpy array. The dimensionality\n",
    "            of the numpy array should be S x A, where S is the number of\n",
    "            states in the environment and A is the number of possible actions.\n",
    "\n",
    "        Returns:\n",
    "          states - (np.array) The sequence of states visited by the agent over\n",
    "            the course of the episode. Does not include the starting state.\n",
    "            Should be of length K, where K is the number of steps taken within\n",
    "            the episode.\n",
    "          actions - (np.array) The sequence of actions taken by the agent over\n",
    "            the course of the episode. Should be of length K, where K is the\n",
    "            number of steps taken within the episode.\n",
    "          rewards - (np.array) The sequence of rewards received by the agent\n",
    "            over the course  of the episode. Should be of length K, where K is\n",
    "            the number of steps taken within the episode.\n",
    "        \"\"\"\n",
    "        states, actions, rewards = [], [], []\n",
    "\n",
    "        state = env.reset()\n",
    "\n",
    "        while True:\n",
    "            action = np.argmax(state_action_values[state])\n",
    "            # take action and observe R, S'\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            # set next state\n",
    "            state = observation\n",
    "            # record data\n",
    "            states.append(observation)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        return np.array(states), np.array(actions), np.array(rewards)\n",
    "\n",
    "    def _get_epsilon(self, progress):\n",
    "        \"\"\"\n",
    "        Retrieves the current value of epsilon. Should be called by the fit\n",
    "        function during each step.\n",
    "\n",
    "        Arguments:\n",
    "            progress - (float) A value between 0 and 1 that indicates the\n",
    "                training progess. Equivalent to current_step / steps.\n",
    "        \"\"\"\n",
    "        return self._adaptive_epsilon(\n",
    "            progress) if self.adaptive else self.epsilon\n",
    "\n",
    "    def _adaptive_epsilon(self, progress):\n",
    "        \"\"\"\n",
    "        An adaptive policy for epsilon-greedy reinforcement learning. Returns\n",
    "        the current epsilon value given the learner's progress. This allows for\n",
    "        the amount of exploratory vs exploitatory behavior to change over time.\n",
    "\n",
    "        See free response question 3 for instructions on how to implement this\n",
    "        function.\n",
    "\n",
    "        Arguments:\n",
    "            progress - (float) A value between 0 and 1 that indicates the\n",
    "                training progess. Equivalent to current_step / steps.\n",
    "        \"\"\"\n",
    "        return (1 - progress) * self.epsilon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
