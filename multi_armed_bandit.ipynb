{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class MultiArmedBandit:\n",
    "    \"\"\"\n",
    "    MultiArmedBandit reinforcement learning agent.\n",
    "\n",
    "    Arguments:\n",
    "      epsilon - (float) The probability of randomly exploring the action space\n",
    "        rather than exploiting the best action.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, epsilon=0.2):\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def fit(self, env, steps=1000):\n",
    "        \"\"\"\n",
    "        Trains the MultiArmedBandit on an OpenAI Gym environment.\n",
    "\n",
    "        Arguments:\n",
    "          env - (Env) An OpenAI Gym environment with discrete actions and\n",
    "            observations. See the OpenAI Gym documentation for example use\n",
    "            cases (https://gym.openai.com/docs/).\n",
    "          steps - (int) The number of actions to perform within the environment\n",
    "            during training.\n",
    "\n",
    "        Returns:\n",
    "          state_action_values - (np.array) The values assigned by the algorithm\n",
    "            to each state-action pair as a 2D numpy array. The dimensionality\n",
    "            of the numpy array should be S x A, where S is the number of\n",
    "            states in the environment and A is the number of possible actions.\n",
    "          rewards - (np.array) A 1D sequence of averaged rewards of length 100.\n",
    "            Let s = np.floor(steps / 100), then rewards[0] should contain the\n",
    "            average reward over the first s steps, rewards[1] should contain\n",
    "            the average reward over the next s steps, etc.\n",
    "        \"\"\"\n",
    "        env.reset()\n",
    "\n",
    "        action_values = np.zeros((env.action_space.n, ))\n",
    "        N_actions_performed = np.zeros((env.action_space.n, ), dtype=int)\n",
    "        rewards = np.zeros((100, ))\n",
    "\n",
    "        s = np.floor(steps / 100)\n",
    "        s_count = 0\n",
    "        reward_sum = 0\n",
    "        idx = 0\n",
    "\n",
    "        for step in range(steps):\n",
    "            # generate random num\n",
    "            p = np.random.random()\n",
    "            # check probability\n",
    "            action = env.action_space.sample(\n",
    "            )  # your agent here (this takes random actions)\n",
    "            if p >= self.epsilon and len(set(action_values)) != 1:\n",
    "                action = np.argmax(action_values)  # take highest Q action\n",
    "            # bandit\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            # update values\n",
    "            N_actions_performed[action] += 1\n",
    "            action_values[action] += 1 / N_actions_performed[action] * (\n",
    "                reward - action_values[action])\n",
    "            reward_sum += reward\n",
    "            # check s\n",
    "            s_count += 1\n",
    "            if s == s_count:\n",
    "                rewards[idx] = reward_sum / (step + 1)\n",
    "                s_count = 0\n",
    "                idx += 1\n",
    "\n",
    "            if done:\n",
    "                observation = env.reset()\n",
    "\n",
    "        # done\n",
    "        return np.repeat([action_values], env.observation_space.n,\n",
    "                         axis=0), rewards\n",
    "\n",
    "    def predict(self, env, state_action_values):\n",
    "        \"\"\"\n",
    "        Runs prediction on an OpenAI environment using the policy defined by\n",
    "        the MultiArmedBandit algorithm and the state action values. Predictions\n",
    "        are run for exactly one episode. Note that one episode may produce a\n",
    "        variable number of steps.\n",
    "\n",
    "        Returns:\n",
    "          states - (np.array) The sequence of states visited by the agent over\n",
    "            the course of the episode. Does not include the starting state.\n",
    "            Should be of length K, where K is the number of steps taken within\n",
    "            the episode.\n",
    "          actions - (np.array) The sequence of actions taken by the agent over\n",
    "            the course of the episode. Should be of length K, where K is the\n",
    "            number of steps taken within the episode.\n",
    "          rewards - (np.array) The sequence of rewards received by the agent\n",
    "            over the course  of the episode. Should be of length K, where K is\n",
    "            the number of steps taken within the episode.\n",
    "        \"\"\"\n",
    "        states, actions, rewards = [], [], []\n",
    "\n",
    "        env.reset()\n",
    "\n",
    "        while True:\n",
    "            action = np.argmax(state_action_values[0])  # take highest Q action\n",
    "            # bandit\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            # record data\n",
    "            states.append(observation)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        return np.array(states), np.array(actions), np.array(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
